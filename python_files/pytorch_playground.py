import torchfrom torch import nnimport torch.nn.functional as Fclass SelfAttention(nn.Module):  def __init__(self, k, heads=8):    super().__init__()    self.k, self.heads = k, heads        # These compute the queries, keys and values for all     # heads (as a single concatenated vector)    self.tokeys    = nn.Linear(k, k * heads, bias=False)    self.toqueries = nn.Linear(k, k * heads, bias=False)    self.tovalues  = nn.Linear(k, k * heads, bias=False)    # This unifies the outputs of the different heads into     # a single k-vector    self.unifyheads = nn.Linear(heads * k, k)      def forward(self, x):    b, t, k = x.size()    h = self.heads    queries = self.toqueries(x).view(b, t, h, k)    keys    = self.tokeys(x)   .view(b, t, h, k)    values  = self.tovalues(x) .view(b, t, h, k)        # - fold heads into the batch dimension    keys = keys.transpose(1, 2).contiguous().view(b * h, t, k)    queries = queries.transpose(1, 2).contiguous().view(b * h, t, k)    values = values.transpose(1, 2).contiguous().view(b * h, t, k)        